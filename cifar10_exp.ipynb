{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4714758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "from kdg.utils import generate_gaussian_parity, generate_ood_samples, generate_spirals, generate_ellipse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow import keras\n",
    "from scipy.io import loadmat\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01eec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    '''\n",
    "    Initialize weights\n",
    "    :param shape: shape of weights, e.g. [w, h ,Cin, Cout] where\n",
    "    w: width of the filters\n",
    "    h: height of the filters\n",
    "    Cin: the number of the channels of the filters\n",
    "    Cout: the number of filters\n",
    "    :return: a tensor variable for weights with initial values\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR WEIGHT_VARIABLE HERE\n",
    "    initial = tf.compat.v1.truncated_normal(shape, stddev=0.1)\n",
    "    W = tf.Variable(initial)\n",
    "    return W\n",
    "#%%\n",
    "def bias_variable(shape):\n",
    "    '''\n",
    "    Initialize biases\n",
    "    :param shape: shape of biases, e.g. [Cout] where\n",
    "    Cout: the number of filters\n",
    "    :return: a tensor variable for biases with initial values\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR BIAS_VARIABLE HERE\n",
    "    initial = tf.compat.v1.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(initial)\n",
    "    return b\n",
    "#%%\n",
    "def conv2d(x, W):\n",
    "    '''\n",
    "    Perform 2-D convolution\n",
    "    :param x: input tensor of size [N, W, H, Cin] where\n",
    "    N: the number of images\n",
    "    W: width of images\n",
    "    H: height of images\n",
    "    Cin: the number of channels of images\n",
    "    :param W: weight tensor [w, h, Cin, Cout]\n",
    "    w: width of the filters\n",
    "    h: height of the filters\n",
    "    Cin: the number of the channels of the filters = the number of channels of images\n",
    "    Cout: the number of filters\n",
    "    :return: a tensor of features extracted by the filters, a.k.a. the results after convolution\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR CONV2D HERE\n",
    "    h_conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return h_conv\n",
    "#%%\n",
    "def max_pool_2x2(x):\n",
    "    '''\n",
    "    Perform non-overlapping 2-D maxpooling on 2x2 regions in the input data\n",
    "    :param x: input data\n",
    "    :return: the results of maxpooling (max-marginalized + downsampling)\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR MAX_POOL_2X2 HERE\n",
    "    h_max = tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    return h_max\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "    '''\n",
    "    Perform non-overlapping 2-D maxpooling on 2x2 regions in the input data\n",
    "    :param x: input data\n",
    "    :return: the results of maxpooling (max-marginalized + downsampling)\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR MAX_POOL_2X2 HERE\n",
    "    h_max = tf.nn.avg_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    return h_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9e931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.W_conv1 = weight_variable([3, 3, 3, 6])\n",
    "        self.b_conv1 = bias_variable([6])\n",
    "        self.W_conv2 = weight_variable([3, 3, 6, 16])\n",
    "        self.b_conv2 = bias_variable([16])\n",
    "        self.W_fc1 = weight_variable([1024, 120])\n",
    "        self.b_fc1 = bias_variable([120])\n",
    "        self.W_fc2 = weight_variable([120, 84])\n",
    "        self.b_fc2 = bias_variable([84])\n",
    "        self.W_fc3 = weight_variable([84, num_classes])\n",
    "        self.b_fc3 = bias_variable([num_classes])\n",
    "\n",
    "        self.vars = [self.W_conv1, self.b_conv1, self.W_conv2, self.b_conv2, \n",
    "                     self.W_fc1, self.b_fc1, self.W_fc2, self.b_fc2, self.W_fc3, self.b_fc3]\n",
    "\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        h_conv1 = tf.nn.relu(conv2d(x, self.W_conv1) + self.b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, self.W_conv2) + self.b_conv2)\n",
    "        h_pool2 = avg_pool_2x2(h_conv2)\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 1024])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, self.W_fc1) + self.b_fc1)\n",
    "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1, self.W_fc2) + self.b_fc2)\n",
    "        h_fc3 = tf.nn.softmax(tf.matmul(h_fc2, self.W_fc3) + self.b_fc3)\n",
    "\n",
    "        return h_fc3\n",
    "\n",
    "def cross_ent(logits, y):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "\n",
    "def max_conf(logits):\n",
    "    y = tf.argmax(logits, 1)\n",
    "    y = tf.one_hot(y, dim)\n",
    "    losses = -tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "def gen_adv(x):\n",
    "    eps = 0.025\n",
    "    n_iters = 20\n",
    "    step_size = 0.02\n",
    "\n",
    "    unif = tf.random.uniform(minval=-eps, maxval=eps, shape=tf.shape(x))\n",
    "    x_adv = x + unif #tf.clip_by_value(x + unif, 0., 1.)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        x_adv = tf.Variable(x_adv)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = max_conf(cnn(x_adv))\n",
    "            grad = tape.gradient(loss, x_adv)\n",
    "            g = tf.sign(grad)\n",
    "\n",
    "        # import pdb;pdb.set_trace()\n",
    "        x_adv_start = x_adv + step_size*g\n",
    "        #x_adv = tf.clip_by_value(x_adv, 0., 1.)\n",
    "        delta = x_adv - x_adv_start\n",
    "        delta = tf.clip_by_value(delta, -eps, eps)\n",
    "        x_adv = x_adv_start + delta\n",
    "\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668f62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess the data ###\n",
    "\n",
    "def get_data(classes):\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    train_idx = np.where(y_train==classes[0])[0]\n",
    "    test_idx = np.where(y_test==classes[0])[0]\n",
    "\n",
    "    for ii in classes[1:]:\n",
    "        train_idx = np.concatenate((\n",
    "                        train_idx,\n",
    "                        np.where(y_train==ii)[0]\n",
    "                    ))\n",
    "        test_idx = np.concatenate((\n",
    "                        test_idx,\n",
    "                        np.where(y_test==ii)[0]\n",
    "                    ))\n",
    "\n",
    "    x_train, y_train = x_train[train_idx], y_train[train_idx]\n",
    "    x_test, y_test = x_test[test_idx], y_test[test_idx]\n",
    "    \n",
    "    _, y_train = np.unique(y_train, return_inverse=True)\n",
    "    _, y_test = np.unique(y_test, return_inverse=True)\n",
    "    \n",
    "    if normalize:\n",
    "        x_train = x_train.astype('float32') / 255\n",
    "        x_test = x_test.astype('float32') / 255\n",
    "    else:\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "    \n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    if subtract_pixel_mean:\n",
    "        x_train -= x_train_mean\n",
    "        x_test -= x_train_mean\n",
    "    return (x_train, y_train), (x_test, y_test), x_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36268359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with classes  [6, 7]  seed  0\n",
      "Epoch 001: loss_main=0.574 loss_acet=-0.685 err=22.50%\n",
      "Epoch 002: loss_main=0.421 loss_acet=-0.669 err=5.00%\n",
      "Epoch 003: loss_main=0.548 loss_acet=-0.687 err=25.00%\n",
      "Epoch 004: loss_main=0.446 loss_acet=-0.687 err=15.00%\n",
      "Epoch 005: loss_main=0.434 loss_acet=-0.685 err=10.00%\n",
      "Epoch 006: loss_main=0.407 loss_acet=-0.689 err=10.00%\n",
      "Epoch 007: loss_main=0.365 loss_acet=-0.691 err=2.50%\n",
      "Epoch 008: loss_main=0.374 loss_acet=-0.691 err=5.00%\n",
      "Epoch 009: loss_main=0.341 loss_acet=-0.691 err=0.00%\n",
      "Epoch 010: loss_main=0.457 loss_acet=-0.690 err=15.00%\n",
      "Epoch 011: loss_main=0.411 loss_acet=-0.688 err=12.50%\n",
      "Epoch 012: loss_main=0.344 loss_acet=-0.691 err=2.50%\n",
      "Epoch 013: loss_main=0.330 loss_acet=-0.691 err=0.00%\n",
      "Epoch 014: loss_main=0.390 loss_acet=-0.692 err=5.00%\n",
      "Epoch 015: loss_main=0.407 loss_acet=-0.692 err=10.00%\n",
      "Epoch 016: loss_main=0.392 loss_acet=-0.690 err=7.50%\n",
      "Epoch 017: loss_main=0.448 loss_acet=-0.692 err=12.50%\n",
      "Epoch 018: loss_main=0.366 loss_acet=-0.691 err=5.00%\n",
      "Epoch 019: loss_main=0.464 loss_acet=-0.692 err=12.50%\n",
      "Epoch 020: loss_main=0.372 loss_acet=-0.692 err=5.00%\n",
      "0.9115\n",
      "({'Task 1': 0.9715831, 'Task 2': 0.9715831, 'Task 3': 0.9715831, 'Task 4': 0.9715831, 'Task 5': 0.9715831, 'cifar100': 0.92913574, 'svhn': 0.8647701, 'noise': 0.9999695}, 0.9115)\n",
      "Training model with classes  [6, 7]  seed  100\n",
      "Epoch 001: loss_main=0.578 loss_acet=-0.677 err=27.50%\n",
      "Epoch 002: loss_main=0.440 loss_acet=-0.681 err=10.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m X_noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform([\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mx_train_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x_train_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x_train_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], x_train_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]],minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acet:\n\u001b[0;32m---> 40\u001b[0m     X_noise \u001b[38;5;241m=\u001b[39m \u001b[43mgen_adv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     42\u001b[0m     logits \u001b[38;5;241m=\u001b[39m cnn(x_train_)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mgen_adv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     52\u001b[0m x_adv \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(x_adv)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 54\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmax_conf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     grad \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, x_adv)\n\u001b[1;32m     56\u001b[0m     g \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msign(grad)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mmax_conf\u001b[0;34m(logits)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_conf\u001b[39m(logits):\n\u001b[1;32m     38\u001b[0m     y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(logits, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy_with_logits(logits\u001b[38;5;241m=\u001b[39mlogits, labels\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(losses)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:4449\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m   4445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_dtype \u001b[38;5;241m!=\u001b[39m off_dtype:\n\u001b[1;32m   4446\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of on_value does not match \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4447\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m of off_value\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(on_dtype, off_dtype))\n\u001b[0;32m-> 4449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moff_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4450\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:6399\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   6397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   6398\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6399\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6400\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOneHot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moff_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maxis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6401\u001b[0m \u001b[43m      \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   6403\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classes_to_consider = [[0,1], [2,3],\n",
    "                       [4,5], [6,7],\n",
    "                       [8,9]]\n",
    "seeds = [0,100,200,300,400]\n",
    "\n",
    "for task, classes in enumerate(classes_to_consider):\n",
    "    subtract_pixel_mean = True\n",
    "    normalize = True\n",
    "    (x_train, y_train), (x_test, y_test), trn_mean = get_data(classes)\n",
    "    input_shape = x_train.shape\n",
    "    batchsize=40\n",
    "    iteration = input_shape[0]//batchsize\n",
    "    epochs = 20\n",
    "    dim=2\n",
    "    #y_train = tf.one_hot(y_train, depth=2)\n",
    "    for seed in seeds:\n",
    "        mmc_dn = {}\n",
    "        acc_dn = 0\n",
    "        \n",
    "        print('Training model with classes ', classes, ' seed ', seed)\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        cnn = LeNet(num_classes=2)\n",
    "        # the default learning rate of Adam might not be the best for this dataset\n",
    "        optimizer = tf.optimizers.Adam(3e-4) \n",
    "\n",
    "        # Training loop\n",
    "        acet = True\n",
    "\n",
    "        for i in range(1, epochs+1):\n",
    "            perm = np.arange(input_shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            perm = perm.reshape(-1,batchsize)\n",
    "\n",
    "            for j in range(iteration):\n",
    "                x_train_ = x_train[perm[j]]\n",
    "                y_train_ = tf.one_hot(y_train[perm[j]], depth=2)\n",
    "                X_noise = tf.random.uniform([2*x_train_.shape[0], x_train_.shape[1], x_train_.shape[2], x_train_.shape[3]],minval=-1,maxval=1)\n",
    "                if acet:\n",
    "                    X_noise = gen_adv(X_noise)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = cnn(x_train_)\n",
    "                    logits_noise = cnn(X_noise)\n",
    "                    loss_main = cross_ent(logits, y_train_)\n",
    "                    loss_acet = acet * max_conf(logits_noise)\n",
    "                    loss = loss_main + loss_acet\n",
    "\n",
    "                grads = tape.gradient(loss, cnn.vars)\n",
    "                optimizer.apply_gradients(zip(grads, cnn.vars))\n",
    "\n",
    "            train_err = np.mean(logits.numpy().argmax(1) != y_train_.numpy().argmax(1))\n",
    "            print(\"Epoch {:03d}: loss_main={:.3f} loss_acet={:.3f} err={:.2%}\".format(i, loss_main, loss_acet, train_err))\n",
    "        \n",
    "        for task_, classes_ in enumerate(classes_to_consider):\n",
    "            subtract_pixel_mean = False\n",
    "            normalize = True\n",
    "            (_, _), (x_test, y_test), _ = get_data(classes)\n",
    "            x_test -= trn_mean\n",
    "            predicted_logits = cnn(x_test)\n",
    "            mmc_dn['Task '+str(task_+1)] = np.mean(np.max(predicted_logits,axis=1))\n",
    "        \n",
    "            if task==task_:\n",
    "                acc_dn = np.mean(predicted_logits.numpy().argmax(1) == y_test)\n",
    "                print(acc_dn)\n",
    "        \n",
    "        (_, _), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "        test_ids =  random.sample(range(0, x_test.shape[0]), 2000)\n",
    "        x_test = x_test[test_ids].astype('float32')/255\n",
    "        x_test -= trn_mean\n",
    "        \n",
    "        predicted_logits = cnn(x_test)\n",
    "        mmc_dn['cifar100'] = np.mean(np.max(predicted_logits,axis=1))\n",
    "        \n",
    "        x_test = loadmat('/Users/jayantadey/svhn/train_32x32.mat')['X']\n",
    "        test_ids =  random.sample(range(0, x_test.shape[3]), 2000)\n",
    "        x_test = x_test[:,:,:,test_ids].astype('float32').reshape(2000,32,32,3)/255\n",
    "        x_test -= trn_mean\n",
    "        \n",
    "        predicted_logits = cnn(x_test)\n",
    "        mmc_dn['svhn'] = np.mean(np.max(predicted_logits,axis=1))\n",
    "        \n",
    "        x_test = np.random.random_integers(0,high=255,size=(2000,32,32,3)).astype('float')\n",
    "        x_test -= trn_mean\n",
    "        predicted_logits = cnn(x_test)\n",
    "        mmc_dn['noise'] = np.mean(np.max(predicted_logits,axis=1))\n",
    "        \n",
    "        summary = (mmc_dn, acc_dn)\n",
    "        \n",
    "        print(summary)\n",
    "        with open('results/ACET_'+str(task)+'_'+str(seed)+'.pickle', 'wb') as f:\n",
    "            pickle.dump(summary,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0fbd1f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8105"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logits.numpy().argmax(1) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1b38348",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cnn(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.max(logits,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10651cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Task 1': 0.7497243,\n",
       " 'Task 2': 0.7497243,\n",
       " 'Task 3': 0.7497243,\n",
       " 'Task 4': 0.7497243,\n",
       " 'Task 5': 0.7497243,\n",
       " 'cifar100': 0.73136854,\n",
       " 'svhn': 0.56476986,\n",
       " 'noise': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmc_dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33907f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/ACET_0_100.pickle'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'results/ACET_'+str(task)+'_'+str(seed)+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ff64cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7735\n"
     ]
    }
   ],
   "source": [
    "for task_, classes_ in enumerate(classes_to_consider):\n",
    "    subtract_pixel_mean = False\n",
    "    normalize = True\n",
    "    (_, _), (x_test, y_test), _ = get_data(classes)\n",
    "    x_test -= trn_mean\n",
    "    predicted_logits = cnn(x_test)\n",
    "    mmc_dn['Task '+str(task_+1)] = np.mean(np.max(predicted_logits,axis=1))\n",
    "\n",
    "    if task==task_:\n",
    "        acc_dn = np.mean(predicted_logits.numpy().argmax(1) == y_test)\n",
    "        print(acc_dn)\n",
    "\n",
    "(_, _), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "test_ids =  random.sample(range(0, x_test.shape[0]), 2000)\n",
    "x_test = x_test[test_ids].astype('float32')/255\n",
    "x_test -= trn_mean\n",
    "\n",
    "predicted_logits = cnn(x_test)\n",
    "mmc_dn['cifar100'] = np.mean(np.max(predicted_logits,axis=1))\n",
    "\n",
    "x_test = loadmat('/Users/jayantadey/svhn/train_32x32.mat')['X']\n",
    "test_ids =  random.sample(range(0, x_test.shape[3]), 2000)\n",
    "x_test = x_test[:,:,:,test_ids].astype('float32').reshape(2000,32,32,3)/255\n",
    "x_test -= trn_mean\n",
    "\n",
    "predicted_logits = cnn(x_test)\n",
    "mmc_dn['svhn'] = np.mean(np.max(predicted_logits,axis=1))\n",
    "\n",
    "x_test = np.random.random_integers(0,high=255,size=(2000,32,32,3)).astype('float')\n",
    "x_test -= trn_mean\n",
    "predicted_logits = cnn(x_test)\n",
    "mmc_dn['noise'] = np.mean(np.max(predicted_logits,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "afe4e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = loadmat('/Users/jayantadey/svhn/train_32x32.mat')['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "69b53910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 33,  84,  19, ...,  92, 190, 216],\n",
       "         [ 30,  76,  54, ...,  78, 188, 217],\n",
       "         [ 38,  59, 110, ..., 101, 191, 212]],\n",
       "\n",
       "        [[ 15,  86,  20, ...,  94, 205, 221],\n",
       "         [ 23,  73,  52, ...,  82, 203, 222],\n",
       "         [ 19,  66, 111, ..., 105, 206, 217]],\n",
       "\n",
       "        [[ 15,  77,  25, ..., 114, 220, 226],\n",
       "         [ 17,  78,  57, ..., 101, 218, 227],\n",
       "         [ 19,  56, 116, ..., 125, 220, 221]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 72,  90,  65, ..., 200, 229, 200],\n",
       "         [ 65,  78, 144, ..., 201, 231, 199],\n",
       "         [ 56,  69, 223, ..., 203, 224, 191]],\n",
       "\n",
       "        [[ 82,  88,  78, ..., 192, 229, 193],\n",
       "         [ 77,  77, 148, ..., 193, 229, 188],\n",
       "         [ 57,  67, 218, ..., 195, 224, 182]],\n",
       "\n",
       "        [[ 89,  88,  98, ..., 190, 229, 197],\n",
       "         [ 79,  78, 158, ..., 191, 228, 189],\n",
       "         [ 59,  66, 220, ..., 193, 223, 186]]],\n",
       "\n",
       "\n",
       "       [[[ 28,  85,  21, ...,  92, 183, 204],\n",
       "         [ 39,  77,  53, ...,  78, 182, 205],\n",
       "         [ 35,  61, 110, ..., 103, 186, 202]],\n",
       "\n",
       "        [[ 14,  83,  19, ...,  93, 200, 210],\n",
       "         [ 25,  73,  52, ...,  80, 199, 211],\n",
       "         [ 22,  64, 106, ..., 106, 201, 208]],\n",
       "\n",
       "        [[ 14,  74,  25, ..., 111, 218, 220],\n",
       "         [ 20,  69,  56, ...,  98, 217, 221],\n",
       "         [ 17,  59, 111, ..., 124, 218, 217]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 40,  89,  63, ..., 181, 227, 201],\n",
       "         [ 39,  82, 137, ..., 180, 228, 199],\n",
       "         [ 50,  64, 208, ..., 184, 223, 193]],\n",
       "\n",
       "        [[ 67,  88,  91, ..., 177, 227, 195],\n",
       "         [ 58,  79, 153, ..., 176, 226, 191],\n",
       "         [ 52,  70, 214, ..., 180, 222, 186]],\n",
       "\n",
       "        [[ 83,  88, 130, ..., 183, 228, 196],\n",
       "         [ 78,  81, 180, ..., 182, 224, 190],\n",
       "         [ 60,  67, 229, ..., 187, 221, 186]]],\n",
       "\n",
       "\n",
       "       [[[ 40,  83,  21, ...,  99, 171, 198],\n",
       "         [ 41,  76,  53, ...,  84, 170, 198],\n",
       "         [ 38,  60, 110, ..., 112, 175, 197]],\n",
       "\n",
       "        [[ 18,  78,  20, ...,  94, 189, 202],\n",
       "         [ 21,  77,  51, ...,  81, 189, 202],\n",
       "         [ 26,  58, 106, ..., 110, 193, 201]],\n",
       "\n",
       "        [[ 16,  61,  22, ..., 107, 213, 212],\n",
       "         [ 17,  50,  52, ...,  94, 213, 211],\n",
       "         [ 23,  54, 106, ..., 123, 215, 210]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 23,  90,  79, ..., 167, 231, 203],\n",
       "         [ 29,  85, 147, ..., 166, 230, 200],\n",
       "         [ 45,  63, 210, ..., 171, 226, 196]],\n",
       "\n",
       "        [[ 35,  88, 125, ..., 172, 229, 198],\n",
       "         [ 42,  83, 181, ..., 171, 226, 194],\n",
       "         [ 44,  66, 230, ..., 176, 223, 191]],\n",
       "\n",
       "        [[ 72,  85, 178, ..., 185, 227, 195],\n",
       "         [ 69,  82, 218, ..., 184, 223, 190],\n",
       "         [ 53,  70, 254, ..., 189, 220, 187]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 86, 100,  88, ...,  99, 187, 233],\n",
       "         [ 81,  98, 162, ...,  94, 185, 226],\n",
       "         [ 75,  72, 237, ..., 110, 186, 228]],\n",
       "\n",
       "        [[ 87,  98,  89, ...,  96, 204, 230],\n",
       "         [ 82,  94, 163, ...,  91, 202, 224],\n",
       "         [ 71,  76, 238, ..., 109, 199, 225]],\n",
       "\n",
       "        [[ 82,  95,  84, ..., 108, 217, 228],\n",
       "         [ 79,  93, 156, ..., 103, 217, 223],\n",
       "         [ 65,  73, 230, ..., 124, 210, 221]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[104, 104,  62, ..., 210, 204, 198],\n",
       "         [104, 104, 142, ..., 207, 200, 196],\n",
       "         [ 87,  86, 227, ..., 204, 195, 190]],\n",
       "\n",
       "        [[104, 102,  67, ..., 206, 196, 184],\n",
       "         [105, 102, 144, ..., 202, 193, 183],\n",
       "         [ 81,  87, 226, ..., 200, 189, 177]],\n",
       "\n",
       "        [[103, 100,  74, ..., 203, 196, 189],\n",
       "         [105, 101, 145, ..., 197, 193, 187],\n",
       "         [ 78,  78, 225, ..., 199, 189, 182]]],\n",
       "\n",
       "\n",
       "       [[[ 84, 103,  88, ...,  94, 186, 231],\n",
       "         [ 86, 104, 164, ...,  91, 184, 226],\n",
       "         [ 64,  79, 240, ..., 103, 185, 228]],\n",
       "\n",
       "        [[ 86, 106,  87, ...,  94, 198, 229],\n",
       "         [ 79, 104, 160, ...,  91, 197, 224],\n",
       "         [ 72,  79, 237, ..., 104, 194, 225]],\n",
       "\n",
       "        [[ 82, 103,  88, ..., 110, 211, 227],\n",
       "         [ 76, 103, 159, ..., 107, 211, 223],\n",
       "         [ 72,  87, 237, ..., 121, 204, 222]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[110, 103,  60, ..., 219, 222, 195],\n",
       "         [103, 104, 141, ..., 218, 216, 194],\n",
       "         [ 84,  86, 230, ..., 215, 212, 186]],\n",
       "\n",
       "        [[106, 103,  61, ..., 218, 214, 181],\n",
       "         [105, 103, 141, ..., 215, 209, 181],\n",
       "         [ 85,  87, 228, ..., 212, 205, 173]],\n",
       "\n",
       "        [[106, 105,  65, ..., 212, 208, 186],\n",
       "         [104,  99, 143, ..., 209, 205, 183],\n",
       "         [ 86,  81, 226, ..., 209, 200, 177]]],\n",
       "\n",
       "\n",
       "       [[[ 85, 103,  84, ...,  88, 190, 230],\n",
       "         [ 88, 106, 160, ...,  87, 188, 226],\n",
       "         [ 68,  82, 238, ...,  94, 190, 227]],\n",
       "\n",
       "        [[ 89, 103,  81, ...,  85, 199, 230],\n",
       "         [ 82, 105, 154, ...,  84, 197, 226],\n",
       "         [ 72,  87, 233, ...,  93, 194, 227]],\n",
       "\n",
       "        [[ 85, 104,  87, ..., 105, 208, 229],\n",
       "         [ 79, 106, 158, ..., 103, 208, 225],\n",
       "         [ 67,  91, 238, ..., 114, 201, 226]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[111, 113,  63, ..., 217, 232, 190],\n",
       "         [104, 103, 144, ..., 217, 227, 190],\n",
       "         [ 87,  88, 235, ..., 214, 223, 181]],\n",
       "\n",
       "        [[109, 104,  62, ..., 221, 226, 178],\n",
       "         [105, 104, 143, ..., 220, 221, 177],\n",
       "         [ 86,  88, 232, ..., 219, 216, 169]],\n",
       "\n",
       "        [[103, 103,  63, ..., 218, 218, 181],\n",
       "         [106,  98, 145, ..., 217, 213, 178],\n",
       "         [ 79,  80, 231, ..., 218, 209, 171]]]], dtype=uint8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53b32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
